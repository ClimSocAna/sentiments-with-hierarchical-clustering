{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f43f2fe-c74b-424e-b251-5260a5471e02",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa452b21-5285-4d60-8a8a-42abc1f59e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# set this if you want to use an Apple Silicon GPU and run into problems\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scc.scc import SCC\n",
    "from scipy.sparse import coo_array, csr_array\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from tqdm import trange\n",
    "\n",
    "from _ctfidf import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0971cc9-acac-4a84-88e5-f581d133a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will enable logging for scc\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e5a22-c1f2-4a28-bf64-81326a612a2d",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f5153-157d-40c7-9c54-09ef433ebb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/tblock/10kGNAD/master/articles.csv\"\n",
    "\n",
    "data = pd.read_csv(url, sep=\";\", header=None, names=[\"category\", \"text\"], on_bad_lines=\"skip\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ec8fb-10c8-4ec1-8c71-407540c3ffa8",
   "metadata": {},
   "source": [
    "### create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac941b3-f0e7-4a4a-a14a-3d9f1b090441",
   "metadata": {},
   "source": [
    "#### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd9c57b-cd4a-4f8b-a952-a290b3926f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbert_cosine = SentenceTransformer(\"deutsche-telekom/gbert-large-paraphrase-cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2b91c-9572-4dba-83e8-640b40130168",
   "metadata": {},
   "source": [
    "#### embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9bce4-0598-4cac-884f-6e67f7675a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gbert_cosine.encode(list(data.text), show_progress_bar=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1e3a5-ce2d-4a99-ba88-e61d52fb7039",
   "metadata": {},
   "source": [
    "### create k-nearest neighbors graph (input for SCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1686b32-b17a-4e8a-9a36-e7a83c152b4e",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d63c4-8508-4cb4-ae63-46c6ef1465e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_top_k(\n",
    "    X: np.ndarray,\n",
    "    top_k: int,\n",
    "    normalize: bool = True,\n",
    "    matrix_format: str = \"csr\",\n",
    "    batch_size: int = 1_000,\n",
    "    show_progress_bar: bool = False,\n",
    "):\n",
    "    \"\"\"This function creates a nearest-neighbhors using Faiss.\"\"\"\n",
    "    top_k = min(top_k, X.shape[0])\n",
    "\n",
    "    if normalize:\n",
    "        X = X.copy()\n",
    "        faiss.normalize_L2(X)\n",
    "    index = faiss.IndexFlatIP(X.shape[1])\n",
    "    \n",
    "    index.add(X)\n",
    "\n",
    "    top_k_data, top_k_col = index.search(X, top_k)\n",
    "\n",
    "    print(\"Search finished.\")\n",
    "\n",
    "    # create canonical form\n",
    "    # do a batched sort over rows to prevent out-of-memory error from sorting with np.lexical\n",
    "    top_k_indices = np.zeros((batch_size, top_k), dtype=np.int32)\n",
    "    for i in trange(0, top_k_col.shape[0], batch_size):\n",
    "        row_start = i\n",
    "        row_end = min(i + batch_size, top_k_col.shape[0])\n",
    "        slice_end = row_end - row_start\n",
    "\n",
    "        top_k_indices[:slice_end] = np.argsort(top_k_col[row_start:row_end], axis=1)\n",
    "\n",
    "        top_k_col[row_start:row_end] = np.take_along_axis(\n",
    "            top_k_col[row_start:row_end], top_k_indices[:slice_end], axis=1\n",
    "        )\n",
    "        top_k_data[row_start:row_end] = np.take_along_axis(\n",
    "            top_k_data[row_start:row_end], top_k_indices[:slice_end], axis=1\n",
    "        )\n",
    "\n",
    "    print(\"Ordering finished.\")\n",
    "\n",
    "    data = top_k_data.flatten()\n",
    "    data = np.sqrt(data, where=data>0)\n",
    "    indices = top_k_col.flatten()\n",
    "    matrix_shape = (X.shape[0], X.shape[0])\n",
    "\n",
    "    if matrix_format == \"csr\":\n",
    "        indptr = np.arange(0, (X.shape[0] + 1) * top_k, top_k)\n",
    "        return csr_array((data, indices, indptr), dtype=np.float32, shape=matrix_shape)\n",
    "    elif matrix_format in [\"coo\", \"dok\", \"lil\"]:\n",
    "        row = np.repeat(np.arange(X.shape[0]), top_k)\n",
    "        coo_matrix = coo_array(\n",
    "            (data, (row, indices)), dtype=np.float32, shape=matrix_shape\n",
    "        )\n",
    "\n",
    "        if matrix_format == \"dok\":\n",
    "            return coo_matrix.todok()\n",
    "        elif matrix_format == \"lil\":\n",
    "            return coo_matrix.tolil()\n",
    "        elif matrix_format == \"csc\":\n",
    "            return coo_matrix.tocsc()\n",
    "        else:\n",
    "            return coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46403af-a593-4875-a33f-fcf42c1f7eba",
   "metadata": {},
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f2764-137c-4cbc-9fde-c164ec59621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 25\n",
    "\n",
    "knn_graph = cosine_top_k(embeddings, top_k=top_k, matrix_format=\"coo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477ae8f-022e-4fa9-9c6f-254126d03a59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### run SCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848288ec-410f-42d1-9cdc-18fb3c265f64",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b5a32-3d00-4d31-984c-f746876903b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = 1.0\n",
    "lower = 0.001\n",
    "num_rounds = 200\n",
    "taus = np.geomspace(start=upper, stop=lower, num=num_rounds)\n",
    "\n",
    "scc = SCC(knn_graph, num_rounds, taus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf7544-e615-43f2-b85e-dcd47c445672",
   "metadata": {},
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ae9fc-68c5-4c92-8c86-f0172e9136bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scc.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e78369-fec0-482b-997a-5a3a9bad5230",
   "metadata": {},
   "source": [
    "#### inspect levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da0748-f056-4e71-b388-5575f9aa4e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, level in enumerate(scc.rounds):\n",
    "    print(i, level.num_uniq_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc3e0d-162e-4cb6-ae65-409a441d6d4b",
   "metadata": {},
   "source": [
    "#### map levels to each other\n",
    "SCC does not ouput relationships between rounds but only gives cluster assignments for each data sample per round.\n",
    "<br> So, we need to extract these relationships. For demonstrational purposes, and similar to the analysis in the paper,\n",
    "<br> we select three `levels` with roughly 20, 100, and 1000 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7487cf-1dd7-4d8c-a71e-6e510214723f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "levels = [167, 109, 38]\n",
    "\n",
    "level_map, inverted_level_map = [], []\n",
    "mapping = None\n",
    "\n",
    "for i, level in enumerate(levels):\n",
    "    if i == 0:\n",
    "        clusters = np.arange(scc.rounds[level].num_uniq_parents)\n",
    "    else:\n",
    "        clusters = np.array([child for children in mapping.values() for child in children])\n",
    "    \n",
    "    if i+1  == len(levels):\n",
    "        level_map.append({cluster: None for cluster in clusters})\n",
    "        break\n",
    "\n",
    "    mapping, inverted_mapping = {}, {}\n",
    "    for cluster in clusters:\n",
    "        cluster_samples = np.where(scc.rounds[level].cluster_assignments==cluster)[0]\n",
    "\n",
    "        children = set(scc.rounds[levels[i+1]].cluster_assignments[cluster_samples])\n",
    "        mapping[cluster] = children\n",
    "\n",
    "        inverted_mapping = inverted_mapping | {child: cluster for child in children}\n",
    "\n",
    "    level_map.append(mapping)  \n",
    "    inverted_level_map.append(inverted_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddece22c-4efb-430f-9d39-c1a54934e8ff",
   "metadata": {},
   "source": [
    "#### extract top key words\n",
    "Here, we extract the top 10 keywords for each cluster (per level). The paper uses a different tokenization strategy\n",
    "<br> specifically designed for German language social media text. For simplicity, we only remove stop_words in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917f06f-a4c3-4173-b130-32a30b4fa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/raw/stop-words-german.txt\"\n",
    "stop_words = list(pd.read_csv(url, header=None).iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce52f1-3577-4e8f-b525-d4d4b20b4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "top_words = []\n",
    "\n",
    "for i, level in enumerate(level_map):\n",
    "    # initialize c-tf-idf vectorizer\n",
    "    # could modify tokenizer here\n",
    "    vectorizer_model = CountVectorizer(lowercase=True, stop_words=stop_words)\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "    # join texts per cluster to prepare for class-based tokenization (similar to BERTopic)\n",
    "    topic_texts = []\n",
    "    clusters = np.array(list(level.keys()))\n",
    "    for cluster in clusters:\n",
    "        cluster_samples = np.where(scc.rounds[levels[i]].cluster_assignments==cluster)[0]\n",
    "        topic_texts.append(\" \".join(data.iloc[cluster_samples].text))\n",
    "    \n",
    "    vectorizer_model.fit(topic_texts)\n",
    "    bow = vectorizer_model.transform(topic_texts)\n",
    "    ctfidf_reprs = ctfidf_model.fit_transform(bow)\n",
    "\n",
    "    # extract top 10 words from sparse matrix (per row = cluster)\n",
    "    words = {}\n",
    "    for row in range(len(ctfidf_reprs.indptr) - 1):\n",
    "        ind_start, ind_end = ctfidf_reprs.indptr[row], ctfidf_reprs.indptr[row + 1]\n",
    "        max_words = min(n_top_words, ind_end - ind_start)\n",
    "        top_word_indices = np.argpartition(\n",
    "            ctfidf_reprs.data[ind_start:ind_end], -max_words\n",
    "        )[-max_words:]\n",
    "        words[clusters[row]] = (\n",
    "            vectorizer_model.get_feature_names_out()[\n",
    "                ctfidf_reprs.indices[ind_start:ind_end][top_word_indices]\n",
    "            ].tolist()\n",
    "        )\n",
    "\n",
    "    top_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f25eb-147a-483f-a91e-c05a7a48e3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82643bf-8529-4a3f-b924-c173684a4a89",
   "metadata": {},
   "source": [
    "### get sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee331fff-7805-4e0b-be63-dea8fb9a511d",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e74dc-57cf-48ad-8241-889cc82eee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will add own model after deanonymization\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "txlm = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44041a5-04a7-4763-a9cc-772d370728a8",
   "metadata": {},
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dab6ca-738e-46de-b561-e5d398e689b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = txlm(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33810149-2d2d-4402-8957-86ca50c05c75",
   "metadata": {},
   "source": [
    "#### export nodes\n",
    "This code combines and transformers the results into a jsonl format (in Python, simply a list of dicts). We used this format\n",
    "<br> for creating the visualization in the paper (see code in the other subfolder in this repo (`treemap_visualization`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ca6a6-775a-41df-be43-20183f7bd35c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodelist = []\n",
    "\n",
    "for i, level in enumerate(levels):\n",
    "    for cluster, children in level_map[i].items():\n",
    "        cluster_samples = np.where(scc.rounds[level].cluster_assignments==cluster)[0]\n",
    "        \n",
    "        name = f\"{level}/{cluster}\"\n",
    "        weight = len(cluster_samples)\n",
    "        keywords = top_words[i][cluster]\n",
    "\n",
    "        sentiment_counts = sentiments.iloc[cluster_samples].sentiment.value_counts()\n",
    "        sentiment_dict = {\n",
    "            \"negative\": int(sentiment_counts.negative) if \"negative\" in sentiment_counts else 0,\n",
    "            \"neutral\": int(sentiment_counts.neutral) if \"neutral\" in sentiment_counts else 0,\n",
    "            \"positive\": int(sentiment_counts.positive) if \"positive\" in sentiment_counts else 0,\n",
    "        \n",
    "        }\n",
    "\n",
    "        nodelist.append({\n",
    "            \"name\": name,\n",
    "            \"label\": None,\n",
    "            \"description\": None,\n",
    "            \"level\": int(level),\n",
    "            \"level_id\": int(cluster),\n",
    "            \"weight\": int(weight),\n",
    "            \"keywords\": keywords,\n",
    "            \"sentiment_dict\": sentiment_dict,\n",
    "            \"parent\": None if i == 0 else int(inverted_level_map[i-1][cluster]),\n",
    "            \"children\": [f'{levels[i+1]}/{child}' for child in children] if children is not None else None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b5b8a-0f11-4cad-b0a3-9eed973d38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da9a36-3a4a-4597-8d25-bc4200e24c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and, optionally, save somwhere\n",
    "save_path = os.path.join(\"path_to_file.jsonl\")\n",
    "\n",
    "with open(save_path, 'w') as f:\n",
    "    for entry in nodelist:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clustering-pipeline] *",
   "language": "python",
   "name": "conda-env-clustering-pipeline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
